{"cells":[{"cell_type":"markdown","metadata":{"id":"MVOGjS1SrXFR"},"source":["# Adverserial Training\n","This is our core work. We adversarially train with three different strategies:\n","1. Adversarial Training with Custom Attack from Pre-Trained Model\n","2. Adversarial Finetuning with Custom Attack from the Initial Hate Speech Model\n","3. Adversarial Training with Textfooler Attack from Pre-Trained Model\n","\n","In the file 'adversarial_attacks.ipynb' these trained hate speech models will be attacked to see if we can achieve any improvements.\n","\n","Following naming will be used below:\n","- <strong>Pre-Trained Model:</strong> This is the [RoBERTa model ](https://huggingface.co/docs/transformers/model_doc/roberta) model from Huggingface\n","- <strong>Initial Hate Speech Model:</strong> This is our RoBERTa model, which we trained on the Hate speech data set.\n","- <strong>Trained Hate Speech Model:</strong> RoBERTa model, which was trained using adversarial training\n","    - <strong>Custom Trained:</strong> RoBERTa model, which was trained using the custom attack\n","    - <strong>Custom Finetuned:</strong> Initial Hate Speech Model, which was finetuned using the custom attack\n","    - <strong>Textfooler Trained:</strong> RoBERTa model, which was trained using the Textfooler attack\n","\n","\n","## Install"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiYQrgLxrXFU"},"outputs":[],"source":["# The kernel needs to be restarted after the pip installs\n","# It appears that textattack has dependency conflicts if not run on an ARM chip.\n","!pip3 install transformers\n","!pip3 install textattack\n","!pip3 install --force-reinstall textattack # force is often needed due to dependency conflicts\n","!pip3 install --upgrade tensorflow\n","!pip3 install sentence_transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VW94E3XrXFV"},"outputs":[],"source":["# This script can help if the nltk downloads fail due to certificate problems.\n","# copied from https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed\n","import nltk\n","import ssl\n","\n","try:\n","    _create_unverified_https_context = ssl._create_unverified_context\n","except AttributeError:\n","    pass\n","else:\n","    ssl._create_default_https_context = _create_unverified_https_context\n","\n","nltk.download()"]},{"cell_type":"markdown","metadata":{"id":"09n0xS1rrXFV"},"source":["## Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vK8N72VSrXFV"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torch\n","import string\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.wsd import lesk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# textattack packages\n","import textattack\n","from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n","from textattack.constraints.semantics import WordEmbeddingDistance\n","\n","# transformers packages\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","\n","from trainer import Trainer"]},{"cell_type":"markdown","metadata":{"id":"VB-4f3ZIrXFV"},"source":["## Load Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grw5W4_6rXFV"},"outputs":[],"source":["# Preprocessing\n","#this is copy from https://www.kaggle.com/code/soumyakushwaha/ethicalcommunicationai\n","# ----------------------------------------\n","stopword = set(stopwords.words('english'))\n","\n","def clean_text(text):\n","    text = str(text).lower()\n","    text = re.sub('\\[.*?\\]', '', text)\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub('<.*?>+', '', text)\n","    text = re.sub(r\"\\@w+|\\#\",'',text)\n","    text = re.sub(r\"[^\\w\\s]\",'',text)\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n","    text = re.sub('\\n', '', text)\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    tweet_tokens = word_tokenize(text)\n","    filtered_tweets=[w for w in tweet_tokens if not w in stopword] #removing stopwords\n","    return \" \".join(filtered_tweets)\n","#--------------------------------------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfGqEXRsrXFY"},"outputs":[],"source":["# Constants\n","SEED = 42\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-5\n","MAX_TEXT_LENGTH = 512\n","EPOCHS = 10\n","MODEL_PATH = 'roberta_model.bin'\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","\n","# Set seeds\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","\n","\n","labeled_data = pd.read_csv('./datasets/hate_speech_data.csv')\n","# Hate Speech and Offensive Language Data: 25.3k total entries.\n","# - Class 0: 1,430 entries (hate speech)\n","# - Class 1: 19,190 entries (offensive language)\n","# - Class 2: 4,163 entries (neither)\n","\n","# Processing labeled hate speech dataset\n","hate_offensive_data = labeled_data[labeled_data['class'] != 2].copy()\n","hate_offensive_data.loc[:, 'category'] = hate_offensive_data['class'].replace([0, 1], 1)\n","hate_offensive_data = hate_offensive_data.rename(columns={'tweet': 'text'})\n","\n","# Test 1 ---\n","# Select data for each class\n","hate_speech_data = labeled_data[labeled_data['class'] == 0].copy()\n","offensive_data = labeled_data[labeled_data['class'] == 1].copy()\n","neither_data = labeled_data[labeled_data['class'] == 2].copy()\n","sample_size = len(hate_speech_data)\n","offensive_sample = offensive_data.sample(n=sample_size, random_state=SEED)\n","neither_sample = neither_data.sample(n=sample_size, random_state=SEED)\n","hate_speech_data['category'] = 1\n","offensive_sample['category'] = 1\n","neither_sample['category'] = 0\n","sampled_data = pd.concat([hate_speech_data, offensive_sample, neither_sample], ignore_index=True)[['tweet', 'category']]\n","sampled_data.rename(columns={'tweet': 'text', 'category': 'label'}, inplace=True)\n","sampled_data['text'] = sampled_data['text'].apply(clean_text)  # Assuming clean_text is a defined function\n","train_data, intermediate_data = train_test_split(sampled_data, test_size=0.3, random_state=SEED)\n","validation_data, test_data = train_test_split(intermediate_data, test_size=0.5, random_state=SEED)\n","train_tokens = tokenizer(train_data['text'].tolist(), padding=True, truncation=True, max_length=MAX_TEXT_LENGTH, return_tensors='pt')\n","validation_tokens = tokenizer(validation_data['text'].tolist(), padding=True, truncation=True, max_length=MAX_TEXT_LENGTH, return_tensors='pt')\n","test_tokens = tokenizer(test_data['text'].tolist(), padding=True, truncation=True, max_length=MAX_TEXT_LENGTH, return_tensors='pt')\n","print(f\"New Train data shape: {train_data.shape}\")\n","print(f\"New Validation data shape: {validation_data.shape}\")\n","print(f\"New Test data shape: {test_data.shape}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Ij6JhbvGrXFY"},"source":["#### Function to load models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpZ9PQYNrXFY"},"outputs":[],"source":["def load_model(file_name):\n","    config = RobertaConfig()\n","    config.num_labels = 2\n","    roberta_base_config = {\n","      \"architectures\": [\n","        \"RobertaForMaskedLM\"\n","      ],\n","      \"attention_probs_dropout_prob\": 0.1,\n","      \"bos_token_id\": 0,\n","      \"eos_token_id\": 2,\n","      \"hidden_act\": \"gelu\",\n","      \"hidden_dropout_prob\": 0.1,\n","      \"hidden_size\": 768,\n","      \"initializer_range\": 0.02,\n","      \"intermediate_size\": 3072,\n","      \"layer_norm_eps\": 1e-05,\n","      \"max_position_embeddings\": 514,\n","      \"model_type\": \"roberta\",\n","      \"num_attention_heads\": 12,\n","      \"num_hidden_layers\": 12,\n","      \"pad_token_id\": 1,\n","      \"type_vocab_size\": 1,\n","      \"vocab_size\": 50265\n","    }\n","\n","    for key in roberta_base_config.keys():\n","        setattr(config, key, roberta_base_config[key])\n","\n","    model = RobertaForSequenceClassification(config)\n","    map_location=torch.device('cpu')\n","    model.load_state_dict(torch.load(file_name, map_location=map_location))\n","    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","    model.eval()\n","    model.to(map_location)\n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"ur5BQMsrrXFZ"},"source":["## Attack Setup\n","Now we prepare the following two attacks for training:\n","\n","- a custom attack\n","- textfooler attack from textattack\n","\n","\n","### Custom Attack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnOeMTvZrXFZ"},"outputs":[],"source":["ATTACK_SEED = 71\n","\n","def create_custom_attack(model_wrapper):\n","\n","    # Define custom attack based on https://textattack.readthedocs.io/en/latest/api/attack.html used for training loop\n","\n","    #UntagetedClassification: An untargeted attack on classification models which attempts\n","    #to minimize the score of the correct label until it is no longer the predicted label.\n","    goal_function = textattack.goal_functions.UntargetedClassification(model_wrapper)\n","\n","    constraints = [\n","        RepeatModification(), # prevents the same word from being modified multiple times\n","        StopwordModification(), # controls the modification of stopwords (e.g., \"the,\" \"is,\" \"and\")\n","        WordEmbeddingDistance(min_cos_sim=0.9), # measures the cosine similarity between word embeddings to ensure that the replacement word is semantically similar\n","    ]\n","\n","    transformation = textattack.transformations.word_swaps.word_swap_embedding.WordSwapEmbedding(max_candidates=50) # (50 is default)\n","    search_method = textattack.search_methods.GreedyWordSwapWIR(wir_method=\"delete\")\n","    custom_attack = textattack.Attack(goal_function, constraints, transformation, search_method) # define the attack\n","\n","    return custom_attack\n"]},{"cell_type":"markdown","metadata":{"id":"f4WJm3u8rXFZ"},"source":["#### Textfooler Attack from testattack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVb7O0btrXFZ"},"outputs":[],"source":["# Use the Bert-attack from textattack based on https://textattack.readthedocs.io/en/latest/3recipes/attack_recipes.html#bert-attack\n","\n","def create_textfooler_attack(model_wrapper):\n","\n","    textfooler_attack = textattack.attack_recipes.textfooler_jin_2019.TextFoolerJin2019.build(model_wrapper) # perform the attack\n","    return textfooler_attack"]},{"cell_type":"markdown","metadata":{"id":"zFl_BynPrXFZ"},"source":["## Train Model on the Attacked Data\n","We now use the attacked data to train our model again. For the training we use the trainer of the textattack library.\n","First we setup the evaluation and training dataset as well as the training arguments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBzAbvGFrXFZ"},"outputs":[],"source":["## Define training base on https://textattack.readthedocs.io/en/latest/api/trainer.html\n","temp = list(validation_data.itertuples(index=False, name=None))\n","eval_dataset = textattack.datasets.Dataset(temp)\n","\n","temp_train = list(train_data.itertuples(index=False, name=None))\n","train_dataset = textattack.datasets.Dataset(temp_train)\n","training_args = textattack.TrainingArgs(\n","    num_epochs=3,\n","    num_clean_epochs=1,\n","    num_train_adv_examples=1000,\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=4,\n","    log_to_tb=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ekHOIh9YrXFZ"},"source":["#### Run Custom Attack Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ftv2MZSrXFZ"},"outputs":[],"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","pretrained_roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n","pretrained_roberta_model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(pretrained_roberta_model, tokenizer)\n","\n","custom_attack = create_custom_attack(pretrained_roberta_model_wrapper)\n","\n","custom_attack_trainer = Trainer(\n","    pretrained_roberta_model_wrapper,\n","    \"classification\",\n","    custom_attack,\n","    train_dataset,\n","    eval_dataset,\n","    training_args\n",")\n","custom_attack_trainer.train()\n","\n","custom_attack_trainer.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"PcOUWwRSrXFZ"},"source":["#### Run Custom Attack Trainer to finetune Initial Hate Speech Model\n","\n","The model was trained according to 'inital_hate_speech_model_training.ipynb' and is reloaded to be finetuned with attacked data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAYYjHIxrXFZ"},"outputs":[],"source":["initial_hate_speech_model = load_model('initial_hate_speech_model.bin')\n","initial_hate_speech_model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(initial_hate_speech_model, tokenizer)\n","\n","custom_attack = create_custom_attack(initial_hate_speech_model_wrapper)\n","\n","custom_attack_trainer_finetune = Trainer(\n","    initial_hate_speech_model_wrapper,\n","    \"classification\",\n","    custom_attack,\n","    train_dataset,\n","    eval_dataset,\n","    training_args\n",")\n","custom_attack_trainer_finetune.train()\n","\n","custom_attack_trainer_finetune.evaluate()"]},{"cell_type":"markdown","metadata":{"id":"HWu6IdBQrXFa"},"source":["#### Run textfooler Attack Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoRUDXMhrXFa"},"outputs":[],"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","pretrained_roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n","pretrained_roberta_model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(pretrained_roberta_model, tokenizer)\n","\n","textfooler_attack = create_textfooler_attack(pretrained_roberta_model_wrapper)\n","\n","\n","textfooler_attack_trainer = textattack.Trainer(\n","    pretrained_roberta_model_wrapper,\n","    \"classification\",\n","    textfooler_attack,\n","    train_dataset,\n","    eval_dataset,\n","    training_args\n",")\n","textfooler_attack_trainer.train()\n","\n","textfooler_attack_trainer.evaluate()"]}],"metadata":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"kernelspec":{"display_name":"Python 3.12.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}