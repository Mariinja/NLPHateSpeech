{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adverserial Training\n",
    "This is our core work. We attack our initial hate speech model to find out our baseline accuracy. \n",
    "After that, we use adversarial training on the pre-trained Roberta model to see if we can \n",
    "improve the accuracy. This trained hate speech model will be attacked again to see if we can achieve any improvements.\n",
    "\n",
    "Following naming will be used below:\n",
    "- <strong>Pre-Trained Model:</strong> This is the [RoBERTa model ](https://huggingface.co/docs/transformers/model_doc/roberta) model from Huggingface\n",
    "- <strong>Initial Hate Speech Model:</strong> This is our RoBERTa model, which we trained on the Hate speech data set.\n",
    "- <strong>Trained Hate Speech Model:</strong> RoBERTa model, which was trained using adversarial training\n",
    "\n",
    "\n",
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers[torch]\n",
    "!pip3 install textattack[tensorflow,optional]\n",
    "#!pip3 install --force-reinstall textattack\n",
    "!pip3 install --upgrade tensorflow\n",
    "#!pip install accelerate -U\n",
    "!pip3 install sentence_transformers\n",
    "!pip3 install pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marinjaprincipe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marinjaprincipe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.wsd import lesk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# textattack packages\n",
    "import textattack\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "\n",
    "# transformers packages\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "from trainer import Trainer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Analysis of the Initial Hate Speech Model\n",
    "As the first step we want to get a baseline of the accuracy of our Initial Hate Speech Model (the training of this model is done in notebook inital_hate_speech_model_training.ipynb). \n",
    "To do so we attack the Initial Hate Speech Model with our custom attack and see how it performes.\n",
    "\n",
    "In a second step, all susccessfull attacks will be used to traine the pre-trained model in order to achieve a better result.\n",
    "\n",
    "#### Data cleaning\n",
    "Since the data needs to be cleaned for the attack, we defined the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is copy from https://www.kaggle.com/code/soumyakushwaha/ethicalcommunicationai\n",
    "# ----------------------------------------\n",
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub(r\"\\@w+|\\#\",'',text)\n",
    "    text = re.sub(r\"[^\\w\\s]\",'',text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    tweet_tokens = word_tokenize(text)\n",
    "    filtered_tweets=[w for w in tweet_tokens if not w in stopword] #removing stopwords\n",
    "    return \" \".join(filtered_tweets)\n",
    "#--------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_TEXT_LENGTH = 512\n",
    "EPOCHS = 10\n",
    "MODEL_PATH = 'roberta_model.bin'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "labeled_data = pd.read_csv('/Users/marinjaprincipe/Documents/UZH/NPL/test/labeled_data 2.csv')\n",
    "# Hate Speech and Offensive Language Data: 25.3k total entries.\n",
    "# - Class 0: 1,430 entries (hate speech)\n",
    "# - Class 1: 19,190 entries (offensive language)\n",
    "# - Class 2: 4,163 entries (neither)\n",
    "\n",
    "# Processing labeled hate speech dataset\n",
    "hate_offensive_data = labeled_data[labeled_data['class'] != 2].copy()\n",
    "hate_offensive_data.loc[:, 'category'] = hate_offensive_data['class'].replace([0, 1], 1)\n",
    "hate_offensive_data = hate_offensive_data.rename(columns={'tweet': 'text'})\n",
    "\n",
    "# Test 1 ---\n",
    "# Select data for each class\n",
    "hate_speech_data = labeled_data[labeled_data['class'] == 0].copy()\n",
    "offensive_data = labeled_data[labeled_data['class'] == 1].copy()\n",
    "neither_data = labeled_data[labeled_data['class'] == 2].copy()\n",
    "sample_size = len(hate_speech_data)\n",
    "offensive_sample = offensive_data.sample(n=sample_size, random_state=SEED)\n",
    "neither_sample = neither_data.sample(n=sample_size, random_state=SEED)\n",
    "hate_speech_data['category'] = 1\n",
    "offensive_sample['category'] = 1\n",
    "neither_sample['category'] = 0\n",
    "sampled_data = pd.concat([hate_speech_data, offensive_sample, neither_sample], ignore_index=True)[['tweet', 'category']]\n",
    "sampled_data.rename(columns={'tweet': 'text', 'category': 'label'}, inplace=True)\n",
    "sampled_data['text'] = sampled_data['text'].apply(clean_text)  # Assuming clean_text is a defined function\n",
    "train_data, intermediate_data = train_test_split(sampled_data, test_size=0.3, random_state=SEED)\n",
    "validation_data, test_data = train_test_split(intermediate_data, test_size=0.5, random_state=SEED)\n",
    "train_tokens = tokenizer(train_data['text'].tolist(), padding=True, truncation=True, max_length=MAX_TEXT_LENGTH, return_tensors='pt')\n",
    "validation_tokens = tokenizer(validation_data['text'].tolist(), padding=True, truncation=True, max_length=MAX_TEXT_LENGTH, return_tensors='pt')\n",
    "test_tokens = tokenizer(test_data['text'].tolist(), padding=True, truncation=True, max_length=MAX_TEXT_LENGTH, return_tensors='pt')\n",
    "print(f\"New Train data shape: {train_data.shape}\")\n",
    "print(f\"New Validation data shape: {validation_data.shape}\")\n",
    "print(f\"New Test data shape: {test_data.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our Initial Hate Speech Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig()\n",
    "config.num_labels = 2\n",
    "roberta_base_config = {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "\n",
    "for key in roberta_base_config.keys():\n",
    "    setattr(config, key, roberta_base_config[key])\n",
    "\n",
    "model = RobertaForSequenceClassification(config)\n",
    "map_location=torch.device('cpu')\n",
    "model.load_state_dict(torch.load('/Users/marinjaprincipe/Documents/UZH/NPL/test/roberta_model.bin', map_location=map_location))\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model.eval()\n",
    "model.to(map_location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Setup\n",
    "Now as we have loaded our trained model, we can attack it. To do so we try different attacks:\n",
    "\n",
    "- a custom attack\n",
    "- the Bert-attack from textattack\n",
    "- bae attack from textattack\n",
    "- textfooler from textattack\n",
    "\n",
    "\n",
    "### Custom Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom attack based on https://textattack.readthedocs.io/en/latest/api/attack.html used for training loop\n",
    "model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "#UntagetedClassification: An untargeted attack on classification models which attempts\n",
    "#to minimize the score of the correct label until it is no longer the predicted label.\n",
    "goal_function = textattack.goal_functions.UntargetedClassification(model_wrapper)\n",
    "\n",
    "constraints = [\n",
    "    RepeatModification(), # prevents the same word from being modified multiple times\n",
    "    StopwordModification(), # controls the modification of stopwords (e.g., \"the,\" \"is,\" \"and\")\n",
    "    WordEmbeddingDistance(min_cos_sim=0.9), # measures the cosine similarity between word embeddings to ensure that the replacement word is semantically similar\n",
    "]\n",
    "\n",
    "transformation = textattack.transformations.word_swaps.word_swap_embedding.WordSwapEmbedding(max_candidates=50) # (50 is default)\n",
    "search_method = textattack.search_methods.GreedyWordSwapWIR(wir_method=\"delete\")\n",
    "custom_attack = textattack.Attack(goal_function, constraints, transformation, search_method) # perform the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.9\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "[Succeeded / Failed / Skipped / Total] 3 / 16 / 1 / 20: 100%|██████████| 20/20 [00:53<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 3      |\n",
      "| Number of failed attacks:     | 16     |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 95.0%  |\n",
      "| Accuracy under attack:        | 80.0%  |\n",
      "| Attack success rate:          | 15.79% |\n",
      "| Average perturbed word %:     | 18.64% |\n",
      "| Average num. words per input: | 9.0    |\n",
      "| Avg num queries:              | 16.95  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<textattack.attack_results.failed_attack_result.FailedAttackResult at 0x325799710>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c7812d0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c391790>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x325269590>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c6b0150>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32d0f6bd0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x32c49bad0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32678aa50>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c7329d0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c6f16d0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c1d7d10>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c637e50>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x32c384350>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x32c717f90>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32d114490>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c272610>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c6c0910>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c52e1d0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c6aac10>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32d36b750>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run attack with defined dataset\n",
    "temp = list(validation_data.itertuples(index=False, name=None))\n",
    "dataset = textattack.datasets.Dataset(temp)\n",
    "\n",
    "# Attack 20 samples with CSV logging and checkpoint saved every 5 interval\n",
    "attack_args = textattack.AttackArgs(num_examples=20, log_to_csv=\"log.csv\", checkpoint_interval=5, checkpoint_dir=\"checkpoints\", disable_stdout=True)\n",
    "custom_attacker = textattack.Attacker(custom_attack, dataset, attack_args)\n",
    "custom_attacker.attack_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert Attack from textattack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Bert-attack from textattack based on https://textattack.readthedocs.io/en/latest/3recipes/attack_recipes.html#bert-attack\n",
    "\n",
    "bert_attack = textattack.attack_recipes.bert_attack_li_2020.BERTAttackLi2020.build(model_wrapper) # perform the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run attack with defined dataset\n",
    "temp = list(validation_data.itertuples(index=False, name=None))\n",
    "dataset = textattack.datasets.Dataset(temp)\n",
    "\n",
    "# Attack 20 samples with CSV logging and checkpoint saved every 5 interval\n",
    "attack_args = textattack.AttackArgs(num_examples=20, log_to_csv=\"log.csv\", checkpoint_interval=5, checkpoint_dir=\"checkpoints\", disable_stdout=True)\n",
    "bert_attacker = textattack.Attacker(bert_attack, dataset, attack_args)\n",
    "bert_attacker.attack_dataset()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bae Attack from testattack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Bert-attack from textattack based on https://textattack.readthedocs.io/en/latest/3recipes/attack_recipes.html#bert-attack\n",
    "\n",
    "bae_attack = textattack.attack_recipes.bae_garg_2019.BAEGarg2019.build(model_wrapper) # perform the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run attack with defined dataset\n",
    "temp = list(validation_data.itertuples(index=False, name=None))\n",
    "dataset = textattack.datasets.Dataset(temp)\n",
    "\n",
    "# Attack 20 samples with CSV logging and checkpoint saved every 5 interval\n",
    "attack_args = textattack.AttackArgs(num_examples=20, log_to_csv=\"log.csv\", checkpoint_interval=5, checkpoint_dir=\"checkpoints\", disable_stdout=True)\n",
    "bae_attacker = textattack.Attacker(bae_attack, dataset, attack_args)\n",
    "bae_attacker.attack_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextFooler Attack from textattack\n",
    "A Strong Baseline for Natural Language Attack on Text Classification and Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Bert-attack from textattack based on https://textattack.readthedocs.io/en/latest/3recipes/attack_recipes.html#bert-attack\n",
    "\n",
    "textFooler_attack = textattack.attack_recipes.textfooler_jin_2019.TextFoolerJin2019.build(model_wrapper) # perform the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run attack with defined dataset\n",
    "temp = list(validation_data.itertuples(index=False, name=None))\n",
    "dataset = textattack.datasets.Dataset(temp)\n",
    "\n",
    "# Attack 20 samples with CSV logging and checkpoint saved every 5 interval\n",
    "attack_args = textattack.AttackArgs(num_examples=20, log_to_csv=\"log.csv\", checkpoint_interval=5, checkpoint_dir=\"checkpoints\", disable_stdout=True)\n",
    "textFooler_attacker = textattack.Attacker(textFooler_attack, dataset, attack_args)\n",
    "textFooler_attacker.attack_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model on the Attacked Data\n",
    "We use now the attacking data to retrain our model again. For the training we use the trainer of the textattack library.\n",
    "First we setup the evaluation and training dataset as well as the training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rt lameassnerd braxton curti hoe ass nigga wen see ima smack like da lil bitch', 1), ('frankiejgrande ew queer white thirsty bitch', 1), ('decodnlyfe lupefiasco larellj another black man anything monkey always monkey chicago idiot', 1), ('rtnba drakes new shoes released nikejordan yes theres glitter shoes dudes fag', 1), ('rt thadisreal ladies nigga always wan na go party every weekend basically side hoe shopping', 1), ('fuck wit us tweakin hoe', 1), ('like really doubt even plays already got lot big man rotation committed bosh bird mcbob amp shawne', 0), ('niggah niggah niggah dont believe wacthh', 1), ('rt harmonlauren jennas faggot', 1), ('rt dignifiedpurity ive work chuckling time derekisnormal said revealing age keeps pussy fresh', 1)]\n",
      "1 <class 'int'>\n",
      "[('species birds reported chesterfield great backyard bird count many area', 0), ('butterfliesblue heard green tea makes lose weight imma coon explains watermelon', 0), ('rt idntwearcondoms u acted like hoe broke im wrong thinking u hoe along', 0), ('rt strocorleone thoughtsofrandy yummysworld deze hoes got past demons em yolo nigga lol hahahahaha', 1), ('joe stop telling pbo job couldnt even keep job teabagger florida morningjoe', 0), ('thought bitch trynna line', 1), ('rt mosoknowso rt troyave got interest tall skinny model bitches like women ass amp hips', 1), ('infidelpamelalc im going blame black man since always blame whitey im equal opportunity hater', 1), ('theomaxximus gerryshalloweenparty halloween yesterday stupid nigger', 1), ('bird got headache', 0)]\n"
     ]
    }
   ],
   "source": [
    "## Defin training base on https://textattack.readthedocs.io/en/latest/api/trainer.html\n",
    "pretrained_roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "temp = list(validation_data.itertuples(index=False, name=None))\n",
    "eval_dataset = textattack.datasets.Dataset(temp)\n",
    "print(temp[:10])\n",
    "print(temp[1][1], type(temp[1][1]))\n",
    "\n",
    "temp_train = list(train_data.itertuples(index=False, name=None))\n",
    "train_dataset = textattack.datasets.Dataset(temp_train)\n",
    "print(temp_train[:10])\n",
    "training_args = textattack.TrainingArgs(\n",
    "    num_epochs=3,\n",
    "    num_clean_epochs=1,\n",
    "    num_train_adv_examples=200,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    log_to_tb=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Custom Attack Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Writing logs to ./outputs/2023-10-19-21-58-08-248160/train_log.txt.\n",
      "textattack: Wrote original training args to ./outputs/2023-10-19-21-58-08-248160/training_args.json.\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "textattack: ***** Running training *****\n",
      "textattack:   Num examples = 3003\n",
      "textattack:   Num epochs = 1\n",
      "textattack:   Num clean epochs = 0\n",
      "textattack:   Instantaneous batch size per device = 1\n",
      "textattack:   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "textattack:   Gradient accumulation steps = 1\n",
      "textattack:   Total optimization steps = 3023\n",
      "textattack: ==========================================================\n",
      "textattack: Epoch 1\n",
      "textattack: Attacking model to generate new adversarial training set...\n",
      "[Succeeded / Failed / Skipped / Total] 20 / 103 / 6 / 129: 100%|██████████| 20/20 [05:31<00:00, 16.58s/it]\n",
      "textattack: Total number of attack results: 129\n",
      "textattack: Attack success rate: 16.26% [20 / 123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.54983: 100%|██████████| 3023/3023 [42:11<00:00,  1.19it/s] \n",
      "textattack: Train accuracy: 72.44%\n",
      "textattack: Eval accuracy: 86.31%\n",
      "textattack: Best score found. Saved model to ./outputs/2023-10-19-21-58-08-248160/best_model/\n",
      "textattack: Wrote README to ./outputs/2023-10-19-21-58-08-248160/README.md.\n",
      "textattack: Eval accuracy: 86.31%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8631415241057543"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_attack_trainer = Trainer(\n",
    "    model_wrapper,\n",
    "    \"classification\",\n",
    "    custom_attack,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    training_args\n",
    ")\n",
    "custom_attack_trainer.train()\n",
    "\n",
    "custom_attack_trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run BERT Attack Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_attack_trainer = textattack.Trainer(\n",
    "    model_wrapper,\n",
    "    \"classification\",\n",
    "    bert_attack,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    training_args\n",
    ")\n",
    "bert_attack_trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Adverserial Trained Models\n",
    "\n",
    "#### Custom Attack Trainer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Eval accuracy: 86.31%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8631415241057543"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_attack_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Logging to CSV at path log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.9\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 3 / 1 / 5:  25%|██▌       | 5/20 [00:12<00:37,  2.50s/it]textattack: Saving checkpoint under \"checkpoints/1697750815263.ta.chkpt\" at 2023-10-19 23:26:55 after 5 attacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 6 / 2 / 10:  50%|█████     | 10/20 [00:21<00:21,  2.15s/it]textattack: Saving checkpoint under \"checkpoints/1697750824246.ta.chkpt\" at 2023-10-19 23:27:04 after 10 attacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 9 / 3 / 15:  75%|███████▌  | 15/20 [00:30<00:10,  2.04s/it]textattack: Saving checkpoint under \"checkpoints/1697750833296.ta.chkpt\" at 2023-10-19 23:27:13 after 15 attacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 13 / 3 / 20: 100%|██████████| 20/20 [00:41<00:00,  2.07s/it]textattack: Saving checkpoint under \"checkpoints/1697750844143.ta.chkpt\" at 2023-10-19 23:27:24 after 20 attacks.\n",
      "[Succeeded / Failed / Skipped / Total] 4 / 13 / 3 / 20: 100%|██████████| 20/20 [00:41<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 4      |\n",
      "| Number of failed attacks:     | 13     |\n",
      "| Number of skipped attacks:    | 3      |\n",
      "| Original accuracy:            | 85.0%  |\n",
      "| Accuracy under attack:        | 65.0%  |\n",
      "| Attack success rate:          | 23.53% |\n",
      "| Average perturbed word %:     | 19.79% |\n",
      "| Average num. words per input: | 9.0    |\n",
      "| Avg num queries:              | 15.18  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<textattack.attack_results.failed_attack_result.FailedAttackResult at 0x31f22a010>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32d266150>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x32b907b90>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x32d00fbd0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x325125190>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x31d3dfd10>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32ae50350>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32d01c550>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x32c115210>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x32d161990>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32644d910>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32d22b890>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x32c03aa90>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c42b2d0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x32c04b610>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32c714450>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x32bc6e5d0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x325127610>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x31dbf2e10>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x32c3403d0>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification(config)\n",
    "map_location=torch.device('cpu')\n",
    "model.load_state_dict(torch.load('/Users/marinjaprincipe/Documents/UZH/NPL/test/outputs/2023-10-19-21-58-08-248160/best_model/pytorch_model.bin', map_location=map_location))\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model.eval()\n",
    "model.to(map_location)\n",
    "# Run attack with defined dataset\n",
    "temp = list(validation_data.itertuples(index=False, name=None))\n",
    "dataset = textattack.datasets.Dataset(temp)\n",
    "\n",
    "# Attack 20 samples with CSV logging and checkpoint saved every 5 interval\n",
    "attack_args = textattack.AttackArgs(num_examples=20, log_to_csv=\"log.csv\", checkpoint_interval=5, checkpoint_dir=\"checkpoints\", disable_stdout=True)\n",
    "custom_attacker = textattack.Attacker(custom_attack, dataset, attack_args)\n",
    "custom_attacker.attack_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert attack trainer evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_attack_trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Aug 24 2023, 15:09:32) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
